{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLP predict whether the review is positive or negative for a given dataset \n",
    "https://drive.google.com/open?id=1-TJWzdxapGhp2aElncd6RH6zOpSAf69X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re # regular expression\n",
    "import string\n",
    "# CountVectorizer used to count words and form matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               Wow... Loved this place.\n",
       "1                                     Crust is not good.\n",
       "2              Not tasty and the texture was just nasty.\n",
       "3      Stopped by during the late May bank holiday of...\n",
       "4      The selection on the menu was great and so wer...\n",
       "5         Now I am getting angry and I want my damn pho.\n",
       "6                  Honeslty it didn't taste THAT fresh.)\n",
       "7      The potatoes were like rubber and you could te...\n",
       "8                              The fries were great too.\n",
       "9                                         A great touch.\n",
       "10                              Service was very prompt.\n",
       "11                                    Would not go back.\n",
       "12     The cashier had no care what so ever on what I...\n",
       "13     I tried the Cape Cod ravoli, chicken, with cra...\n",
       "14     I was disgusted because I was pretty sure that...\n",
       "15     I was shocked because no signs indicate cash o...\n",
       "16                                   Highly recommended.\n",
       "17                Waitress was a little slow in service.\n",
       "18     This place is not worth your time, let alone V...\n",
       "19                                  did not like at all.\n",
       "20                                   The Burrittos Blah!\n",
       "21                                    The food, amazing.\n",
       "22                                 Service is also cute.\n",
       "23     I could care less... The interior is just beau...\n",
       "24                                    So they performed.\n",
       "25     That's right....the red velvet cake.....ohhh t...\n",
       "26            - They never brought a salad we asked for.\n",
       "27     This hole in the wall has great Mexican street...\n",
       "28     Took an hour to get our food only 4 tables in ...\n",
       "29                     The worst was the salmon sashimi.\n",
       "                             ...                        \n",
       "970    I immediately said I wanted to talk to the man...\n",
       "971                      The ambiance isn't much better.\n",
       "972    Unfortunately, it only set us up for disapppoi...\n",
       "973                                The food wasn't good.\n",
       "974    Your servers suck, wait, correction, our serve...\n",
       "975        What happened next was pretty....off putting.\n",
       "976    too bad cause I know it's family owned, I real...\n",
       "977                 Overpriced for what you are getting.\n",
       "978                 I vomited in the bathroom mid lunch.\n",
       "979    I kept looking at the time and it had soon bec...\n",
       "980    I have been to very few places to eat that und...\n",
       "981    We started with the tuna sashimi which was bro...\n",
       "982                              Food was below average.\n",
       "983    It sure does beat the nachos at the movies but...\n",
       "984         All in all, Ha Long Bay was a bit of a flop.\n",
       "985    The problem I have is that they charge $11.99 ...\n",
       "986    Shrimp- When I unwrapped it (I live only 1/2 a...\n",
       "987       It lacked flavor, seemed undercooked, and dry.\n",
       "988    It really is impressive that the place hasn't ...\n",
       "989    I would avoid this place if you are staying in...\n",
       "990    The refried beans that came with my meal were ...\n",
       "991           Spend your money and time some place else.\n",
       "992    A lady at the table next to us found a live gr...\n",
       "993              the presentation of the food was awful.\n",
       "994             I can't tell you how disappointed I was.\n",
       "995    I think food should have flavor and texture an...\n",
       "996                             Appetite instantly gone.\n",
       "997    Overall I was not impressed and would not go b...\n",
       "998    The whole experience was underwhelming, and I ...\n",
       "999    Then, as if I hadn't wasted enough of my life ...\n",
       "Name: Review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", sep='\\t', encoding='utf-8')\n",
    "dataset.Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review    False\n",
       "Liked     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>996.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.501004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Liked\n",
       "count  996.000000\n",
       "mean     0.501004\n",
       "std      0.500250\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is a collection of text\n",
    "# data cleaning steps\n",
    "# 1. Remove punctuation, 2. Remove numbers and 3. Lowercase letters\n",
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_and_stemming(text):\n",
    "    text = text.lower()  # converting text into lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)  # removing .,*,?,.... symbols\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # revoming punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  # removing numbers\n",
    "    words = text.split()  # spliting sentence into words\n",
    "    # stemming is the process of extracting root word from a word\n",
    "    # stop words are those which express state of words like a, an, as, those, when etc.\n",
    "    stem_words = [ porter.stem(word) for word in words if word not in set( nltk.corpus.stopwords.words('english') ) ]\n",
    "    stemmed_sentence = ' '.join(stem_words)\n",
    "    return stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-aranging reviews with cleaned review\n",
    "cleaned_review = lambda review: cleaning_and_stemming(review)\n",
    "dataset.Review = dataset.Review.apply(cleaned_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow love place</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasti textur nasti</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select menu great price</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                                     wow love place      1\n",
       "1                                         crust good      0\n",
       "2                                 tasti textur nasti      0\n",
       "3  stop late may bank holiday rick steve recommen...      1\n",
       "4                            select menu great price      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It consists three steps \n",
    "# Clean Text - remove excess, unnecessary parts of the text\n",
    "# Tokenize Text - split the text into smaller pieces\n",
    "# Document-Term Matrix - put into a matrix so a machine can read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CountVectorizer( max_features = 10000 )\n",
    "tokenized_data = tokenizer.fit_transform( dataset.Review )\n",
    "document_term_matrix = pd.DataFrame( tokenized_data.toarray(), columns=tokenizer.get_feature_names() )\n",
    "document_term_matrix.index = dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolut</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>accid</th>\n",
       "      <th>accommod</th>\n",
       "      <th>accomod</th>\n",
       "      <th>accordingli</th>\n",
       "      <th>account</th>\n",
       "      <th>ach</th>\n",
       "      <th>acknowledg</th>\n",
       "      <th>across</th>\n",
       "      <th>...</th>\n",
       "      <th>yelper</th>\n",
       "      <th>yet</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>your</th>\n",
       "      <th>yucki</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummi</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolut  absolutley  accid  accommod  accomod  accordingli  account  ach  \\\n",
       "0        0           0      0         0        0            0        0    0   \n",
       "1        0           0      0         0        0            0        0    0   \n",
       "2        0           0      0         0        0            0        0    0   \n",
       "3        0           0      0         0        0            0        0    0   \n",
       "4        0           0      0         0        0            0        0    0   \n",
       "\n",
       "   acknowledg  across  ...   yelper  yet  youd  youll  your  yucki  yukon  \\\n",
       "0           0       0  ...        0    0     0      0     0      0      0   \n",
       "1           0       0  ...        0    0     0      0     0      0      0   \n",
       "2           0       0  ...        0    0     0      0     0      0      0   \n",
       "3           0       0  ...        0    0     0      0     0      0      0   \n",
       "4           0       0  ...        0    0     0      0     0      0      0   \n",
       "\n",
       "   yum  yummi  zero  \n",
       "0    0      0     0  \n",
       "1    0      0     0  \n",
       "2    0      0     0  \n",
       "3    0      0     0  \n",
       "4    0      0     0  \n",
       "\n",
       "[5 rows x 1608 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = document_term_matrix\n",
    "y = dataset['Liked']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(n_estimators = 300)\n",
    "random_forest_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_vector_classifier = SVC()\n",
    "support_vector_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "decision_tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_prediction = random_forest_classifier.predict(X_test)\n",
    "lr_prediction = logistic_regression.predict(X_test)\n",
    "svc_prediction = support_vector_classifier.predict(X_test)\n",
    "decision_tree_prediction = decision_tree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest: 76.58862876254182\n",
      "Logistic Regression: 80.60200668896321\n",
      "Support Vector: 48.49498327759198\n",
      "Decision Tree: 72.90969899665552\n"
     ]
    }
   ],
   "source": [
    "print(\"Random forest:\", accuracy_score(rfc_prediction, y_test)*100)\n",
    "print(\"Logistic Regression:\", accuracy_score(lr_prediction, y_test)*100)\n",
    "print(\"Support Vector:\", accuracy_score(svc_prediction, y_test)*100)\n",
    "print(\"Decision Tree:\", accuracy_score(decision_tree_prediction, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "[[121  46]\n",
      " [ 24 108]]\n",
      "Logistic Regression:\n",
      "[[121  34]\n",
      " [ 24 120]]\n",
      "Support Vector:\n",
      "[[145 154]\n",
      " [  0   0]]\n",
      "Decision Tree:\n",
      "[[108  44]\n",
      " [ 37 110]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Random forest:\")\n",
    "print(confusion_matrix(rfc_prediction, y_test))\n",
    "print(\"Logistic Regression:\")\n",
    "print(confusion_matrix(lr_prediction, y_test))\n",
    "print(\"Support Vector:\")\n",
    "print(confusion_matrix(svc_prediction, y_test))\n",
    "print(\"Decision Tree:\")\n",
    "print(confusion_matrix(decision_tree_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.72      0.78       167\n",
      "           1       0.70      0.82      0.76       132\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       299\n",
      "   macro avg       0.77      0.77      0.77       299\n",
      "weighted avg       0.78      0.77      0.77       299\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81       155\n",
      "           1       0.78      0.83      0.81       144\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       299\n",
      "   macro avg       0.81      0.81      0.81       299\n",
      "weighted avg       0.81      0.81      0.81       299\n",
      "\n",
      "Support Vector:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.65       299\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       299\n",
      "   macro avg       0.50      0.24      0.33       299\n",
      "weighted avg       1.00      0.48      0.65       299\n",
      "\n",
      "Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73       152\n",
      "           1       0.71      0.75      0.73       147\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       299\n",
      "   macro avg       0.73      0.73      0.73       299\n",
      "weighted avg       0.73      0.73      0.73       299\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Random forest:\")\n",
    "print(classification_report(rfc_prediction, y_test))\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(lr_prediction, y_test))\n",
    "print(\"Support Vector:\")\n",
    "print(classification_report(svc_prediction, y_test))\n",
    "print(\"Decision Tree:\")\n",
    "print(classification_report(decision_tree_prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
